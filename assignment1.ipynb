{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Programming assignment 1\n",
    "\n",
    "By Group 5: Torbjörn Livén and Samuel Kajava\n",
    "\n",
    "## Task 1. Fetal heart condition diagnosis\n",
    "\n",
    "In this task we explore different classifiers on the CTG dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-18 10:32:26--  https://www.cse.chalmers.se/~richajo/dit866/data/CTG.csv\r\n",
      "Resolving www.cse.chalmers.se (www.cse.chalmers.se)... 129.16.221.33\r\n",
      "Connecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.221.33|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 307385 (300K) [text/plain]\r\n",
      "Saving to: ‘CTG.csv.1’\r\n",
      "\r\n",
      "CTG.csv.1           100%[===================>] 300.18K   406KB/s    in 0.7s    \r\n",
      "\r\n",
      "2024-01-18 10:32:26 (406 KB/s) - ‘CTG.csv.1’ saved [307385/307385]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Fetch the CTG data set.\n",
    "!wget https://www.cse.chalmers.se/~richajo/dit866/data/CTG.csv"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U9sAB53kuUgI",
    "outputId": "8329e166-658f-4035-b3aa-00e7bb69351b",
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:26.912368Z",
     "start_time": "2024-01-18T09:32:25.979505Z"
    }
   },
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Reading the data\n",
    "\n",
    "This code was taken from the assignment description and reads the data into training and testing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "UHxGFV10txRs",
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:26.928242Z",
     "start_time": "2024-01-18T09:32:26.915334Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the CSV file.\n",
    "data = pd.read_csv('CTG.csv', skiprows=1)\n",
    "\n",
    "# Select the relevant numerical columns.\n",
    "selected_cols = ['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'ASTV', 'MSTV', 'ALTV',\n",
    "                 'MLTV', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean',\n",
    "                 'Median', 'Variance', 'Tendency', 'NSP']\n",
    "data = data[selected_cols].dropna()\n",
    "\n",
    "# Shuffle the dataset.\n",
    "data_shuffled = data.sample(frac=1.0, random_state=0)\n",
    "\n",
    "# Split into input part X and output part Y.\n",
    "X = data_shuffled.drop('NSP', axis=1)\n",
    "\n",
    "\n",
    "# Map the diagnosis code to a human-readable label.\n",
    "def to_label(y):\n",
    "    return [None, 'normal', 'suspect', 'pathologic'][(int(y))]\n",
    "\n",
    "\n",
    "Y = data_shuffled['NSP'].apply(to_label)\n",
    "\n",
    "# Partition the data into training and test sets.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "MmSVkGygupV3",
    "outputId": "90c99e15-d729-4269-cfac-7986e77e5e61",
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:26.952148Z",
     "start_time": "2024-01-18T09:32:26.934542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         LB   AC   FM   UC   DL   DS   DP  ASTV  MSTV  ALTV  ...  Width  \\\n658   130.0  1.0  0.0  3.0  0.0  0.0  0.0  24.0   1.2  12.0  ...   35.0   \n1734  134.0  9.0  1.0  8.0  5.0  0.0  0.0  59.0   1.2   0.0  ...  109.0   \n1226  125.0  1.0  0.0  4.0  0.0  0.0  0.0  43.0   0.7  31.0  ...   21.0   \n1808  143.0  0.0  0.0  1.0  0.0  0.0  0.0  69.0   0.3   6.0  ...   27.0   \n825   152.0  0.0  0.0  4.0  0.0  0.0  0.0  62.0   0.4  59.0  ...   25.0   \n\n        Min    Max  Nmax  Nzeros   Mode   Mean  Median  Variance  Tendency  \n658   120.0  155.0   1.0     0.0  134.0  133.0   135.0       1.0       0.0  \n1734   80.0  189.0   6.0     0.0  150.0  146.0   150.0      33.0       0.0  \n1226  120.0  141.0   0.0     0.0  131.0  130.0   132.0       1.0       0.0  \n1808  132.0  159.0   1.0     0.0  145.0  144.0   146.0       1.0       0.0  \n825   136.0  161.0   0.0     0.0  159.0  156.0   158.0       1.0       1.0  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LB</th>\n      <th>AC</th>\n      <th>FM</th>\n      <th>UC</th>\n      <th>DL</th>\n      <th>DS</th>\n      <th>DP</th>\n      <th>ASTV</th>\n      <th>MSTV</th>\n      <th>ALTV</th>\n      <th>...</th>\n      <th>Width</th>\n      <th>Min</th>\n      <th>Max</th>\n      <th>Nmax</th>\n      <th>Nzeros</th>\n      <th>Mode</th>\n      <th>Mean</th>\n      <th>Median</th>\n      <th>Variance</th>\n      <th>Tendency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>658</th>\n      <td>130.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>24.0</td>\n      <td>1.2</td>\n      <td>12.0</td>\n      <td>...</td>\n      <td>35.0</td>\n      <td>120.0</td>\n      <td>155.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>134.0</td>\n      <td>133.0</td>\n      <td>135.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1734</th>\n      <td>134.0</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>59.0</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>109.0</td>\n      <td>80.0</td>\n      <td>189.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>150.0</td>\n      <td>146.0</td>\n      <td>150.0</td>\n      <td>33.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1226</th>\n      <td>125.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>43.0</td>\n      <td>0.7</td>\n      <td>31.0</td>\n      <td>...</td>\n      <td>21.0</td>\n      <td>120.0</td>\n      <td>141.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>131.0</td>\n      <td>130.0</td>\n      <td>132.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1808</th>\n      <td>143.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>69.0</td>\n      <td>0.3</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>27.0</td>\n      <td>132.0</td>\n      <td>159.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>145.0</td>\n      <td>144.0</td>\n      <td>146.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>825</th>\n      <td>152.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>62.0</td>\n      <td>0.4</td>\n      <td>59.0</td>\n      <td>...</td>\n      <td>25.0</td>\n      <td>136.0</td>\n      <td>161.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>159.0</td>\n      <td>156.0</td>\n      <td>158.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peak at the data.\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWFCIyBevAkH"
   },
   "source": [
    "### Step 2. Training the baseline classifier\n",
    "\n",
    "We begin by using a dummy classifier as a baseline for our upcomping implementation. A higher aggregated result means that the accuracy is higher, so we want to find a classifier with a good aggregated result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:26.958191Z",
     "start_time": "2024-01-18T09:32:26.937834Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Create dummy classifier.\n",
    "clf = DummyClassifier(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:26.961121Z",
     "start_time": "2024-01-18T09:32:26.941016Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross validation.\n",
    "dummy_cross_val = cross_val_score(clf, Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have cross validated the scores, we now aggregate the results to make it comparable to other classifiers. We use the mean to find a good representation of the results."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.7805882352941176"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Helper that aggregates the results for an array of scores.\n",
    "def aggregate(arr: np.ndarray):\n",
    "    return np.mean(arr)\n",
    "\n",
    "\n",
    "# Aggregate and print the dummy baseline score.\n",
    "dummy_aggr = aggregate(dummy_cross_val)\n",
    "dummy_aggr\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:26.971065Z",
     "start_time": "2024-01-18T09:32:26.948831Z"
    }
   },
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Trying out different classifiers\n",
    "\n",
    "In this step, we use a number of classifiers and compare the aggregated results using the `aggregate()` function defined above. We choose to scale the data to help the linear classifiers converge [[1]](https://scikit-learn.org/stable/modules/preprocessing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:26.973991Z",
     "start_time": "2024-01-18T09:32:26.951946Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the data.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:45.989299Z",
     "start_time": "2024-01-18T09:32:26.958931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:\n",
      "---------\n",
      "Dummy: 0.7805882352941176\n",
      "\n",
      "Tree-based:\n",
      "-----------\n",
      "Decision tree: 0.9194117647058823\n",
      "Random forest: 0.9376470588235295\n",
      "Gradient boost: 0.9452941176470588\n",
      "\n",
      "Linear:\n",
      "-------\n",
      "Perceptron: 0.8729411764705883\n",
      "Logistic regression: 0.891764705882353\n",
      "Linear SVC: 0.8905882352941177\n",
      "\n",
      "Neural net:\n",
      "-----------\n",
      "MLP: 0.9311764705882354\n"
     ]
    }
   ],
   "source": [
    "import sklearn.tree as tree\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.linear_model as linear\n",
    "import sklearn.svm as svm\n",
    "import sklearn.neural_network as nn\n",
    "\n",
    "# Run cross validation on decision tree, random forest, gradient boosting, perceptron, logistic regression, linear SVC and MLP.\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_cross_val = cross_val_score(tree_clf, Xtrain, Ytrain)\n",
    "\n",
    "forest_clf = ensemble.RandomForestClassifier()\n",
    "forest_cross_val = cross_val_score(forest_clf, Xtrain, Ytrain)\n",
    "\n",
    "gb_clf = ensemble.GradientBoostingClassifier(max_depth=10)\n",
    "gb_cross_val = cross_val_score(gb_clf, Xtrain, Ytrain)\n",
    "\n",
    "perceptron_clf = linear.Perceptron()\n",
    "perceptron_cross_val = cross_val_score(perceptron_clf, Xtrain, Ytrain)\n",
    "\n",
    "logreg_clf = linear.LogisticRegression()\n",
    "logreg_cross_val = cross_val_score(logreg_clf, Xtrain, Ytrain)\n",
    "\n",
    "linsvc_clf = svm.LinearSVC(dual=False)\n",
    "linsvc_cross_val = cross_val_score(linsvc_clf, Xtrain, Ytrain)\n",
    "\n",
    "mlp_clf = nn.MLPClassifier(max_iter=3000, hidden_layer_sizes=(100, 100), solver='adam')\n",
    "mlp_cross_val = cross_val_score(mlp_clf, Xtrain, Ytrain)\n",
    "\n",
    "# Aggregate the results.\n",
    "tree_aggr = aggregate(tree_cross_val)\n",
    "forest_aggr = aggregate(forest_cross_val)\n",
    "gb_aggr = aggregate(gb_cross_val)\n",
    "perceptron_aggr = aggregate(perceptron_cross_val)\n",
    "logreg_aggr = aggregate(logreg_cross_val)\n",
    "linsvc_aggr = aggregate(linsvc_cross_val)\n",
    "mlp_aggr = aggregate(mlp_cross_val)\n",
    "\n",
    "# Print the results.\n",
    "print('Baseline:\\n---------')\n",
    "print('Dummy:', dummy_aggr)\n",
    "\n",
    "print('\\nTree-based:\\n-----------')\n",
    "print('Decision tree:', tree_aggr)\n",
    "print('Random forest:', forest_aggr)\n",
    "print('Gradient boost:', gb_aggr)\n",
    "\n",
    "print('\\nLinear:\\n-------')\n",
    "print('Perceptron:', perceptron_aggr)\n",
    "print('Logistic regression:', logreg_aggr)\n",
    "print('Linear SVC:', linsvc_aggr)\n",
    "\n",
    "print('\\nNeural net:\\n-----------')\n",
    "print('MLP:', mlp_aggr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Choosing the best classifier\n",
    "\n",
    "With the cross validation results finished and aggregated, we choose the best performing one based on their scores. The winner here is gradient boost, and [`report.md`](./report.md) contains more discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:48.775615Z",
     "start_time": "2024-01-18T09:32:46.009089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Gradient boost\n",
      "Accuracy: 0.9295774647887324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Candidate models and their aggregation results.\n",
    "candidates = {\n",
    "    'Decision tree': (tree_clf, tree_aggr),\n",
    "    'Random forest': (forest_clf, forest_aggr),\n",
    "    'Gradient boost': (gb_clf, gb_aggr),\n",
    "    'Perceptron': (perceptron_clf, perceptron_aggr),\n",
    "    'Logistic regression': (logreg_clf, logreg_aggr),\n",
    "    'Linear SVC': (linsvc_clf, linsvc_aggr),\n",
    "    'MLP': (mlp_clf, mlp_aggr)\n",
    "}\n",
    "\n",
    "# Find the best model.\n",
    "best = None\n",
    "for candidate in candidates:\n",
    "    if best is None or candidates[candidate][1] > candidates[best][1]:\n",
    "        best = candidate\n",
    "\n",
    "print('Best model:', best)\n",
    "best_clf = candidates[best][0]\n",
    "best_clf.fit(Xtrain, Ytrain)\n",
    "Yguess = best_clf.predict(Xtest)\n",
    "print('Accuracy:', accuracy_score(Ytest, Yguess))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "When running the classifiers inlcuded in the examples from the assignment description, gradient boosting performed best of the bunch. We concluded this by comparing the aggregated results from each classifier.\n",
    "\n",
    "To explain the gradient boosting classifier, we first discuss the concept of gradient boosting. Boosting is a method where you train a model several times while iteratively trying to correct the previous iteration. This process is described as a way of combining weak learners into strong ones [[1]](https://en.wikipedia.org/wiki/Boosting_(machine_learning)). Gradient boosting utilizes this concept by training each model to minmize the loss function first, followed by training a new model to minimize the gradient. This process successively adds predicitions to the ensemble. These predictions are done by regression trees and return the final predictions once the `criterion` hyperparameter is met. In our case, we used the default `friedman_mse` which measures the quality of the iteration by looking at \"the mean squared error with improvement score by Friedman\" [[2]](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Decision trees\n",
    "\n",
    "The code for this task was provided by the assignment description, and was taken from this [page](https://www.cse.chalmers.se/~richajo/dit866/lectures/l1/Lecture%201.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:48.780784Z",
     "start_time": "2024-01-18T09:32:48.776417Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTreeLeaf:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    # This method computes the prediction for this leaf node. This will just return a constant value.\n",
    "    def predict(self, x):\n",
    "        return self.value\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_id = str(node_counter)\n",
    "        val_str = f'{self.value:.4g}' if isinstance(\n",
    "            self.value, float) else str(self.value)\n",
    "        graph.node(node_id, val_str, style='filled')\n",
    "        return node_counter + 1, node_id\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, DecisionTreeLeaf):\n",
    "            return self.value == other.value\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:48.783128Z",
     "start_time": "2024-01-18T09:32:48.779703Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTreeBranch:\n",
    "    def __init__(self, feature, threshold, low_subtree, high_subtree):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.low_subtree = low_subtree\n",
    "        self.high_subtree = high_subtree\n",
    "\n",
    "    # For a branch node, we compute the prediction by first considering the feature, and then\n",
    "    # calling the upper or lower subtree, depending on whether the feature is or isn't greater\n",
    "    # than the threshold.\n",
    "    def predict(self, x):\n",
    "        if x[self.feature] <= self.threshold:\n",
    "            return self.low_subtree.predict(x)\n",
    "        else:\n",
    "            return self.high_subtree.predict(x)\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_counter, low_id = self.low_subtree.draw_tree(\n",
    "            graph, node_counter, names)\n",
    "        node_counter, high_id = self.high_subtree.draw_tree(\n",
    "            graph, node_counter, names)\n",
    "        node_id = str(node_counter)\n",
    "        fname = f'F{self.feature}' if names is None else names[self.feature]\n",
    "        lbl = f'{fname} > {self.threshold:.4g}?'\n",
    "        graph.node(node_id, lbl, shape='box',\n",
    "                   fillcolor='yellow', style='filled, rounded')\n",
    "        graph.edge(node_id, low_id, 'False')\n",
    "        graph.edge(node_id, high_id, 'True')\n",
    "        return node_counter + 1, node_id"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class DecisionTree(ABC, BaseEstimator):\n",
    "    def __init__(self, max_depth):\n",
    "        super().__init__()\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    # As usual in scikit-learn, the training method is called *fit*. We first process the dataset so that\n",
    "    # we're sure that it's represented as a NumPy matrix. Then we call the recursive tree-building method\n",
    "    # called make_tree (see below).\n",
    "    def fit(self, X, Y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.names = X.columns\n",
    "            X = X.to_numpy()\n",
    "        elif isinstance(X, list):\n",
    "            self.names = None\n",
    "            X = np.array(X)\n",
    "        else:\n",
    "            self.names = None\n",
    "        Y = np.array(Y)\n",
    "        self.root = self.make_tree(X, Y, self.max_depth)\n",
    "\n",
    "    def draw_tree(self):\n",
    "        graph = Digraph()\n",
    "        self.root.draw_tree(graph, 0, self.names)\n",
    "        return graph\n",
    "\n",
    "    # By scikit-learn convention, the method *predict* computes the classification or regression output\n",
    "    # for a set of instances.\n",
    "    # To implement it, we call a separate method that carries out the prediction for one instance.\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        return [self.predict_one(x) for x in X]\n",
    "\n",
    "    # Predicting the output for one instance.\n",
    "    def predict_one(self, x):\n",
    "        return self.root.predict(x)\n",
    "\n",
    "        # This is the recursive training \n",
    "\n",
    "    def make_tree(self, X, Y, max_depth):\n",
    "\n",
    "        # We start by computing the default value that will be used if we'll return a leaf node.\n",
    "        # For classifiers, this will be the most common value in Y.\n",
    "        default_value = self.get_default_value(Y)\n",
    "\n",
    "        # First the two base cases in the recursion: is the training set completely\n",
    "        # homogeneous, or have we reached the maximum depth? Then we need to return a leaf.\n",
    "\n",
    "        # If we have reached the maximum depth, return a leaf with the majority value.\n",
    "        if max_depth == 0:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # If all the instances in the remaining training set have the same output value,\n",
    "        # return a leaf with this value.\n",
    "        if self.is_homogeneous(Y):\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Select the \"most useful\" feature and split threshold. To rank the \"usefulness\" of features,\n",
    "        # we use one of the classification or regression criteria.\n",
    "        # For each feature, we call best_split (defined in a subclass). We then maximize over the features.\n",
    "        n_features = X.shape[1]\n",
    "        _, best_feature, best_threshold = max(self.best_split(X, Y, feature) for feature in range(n_features))\n",
    "\n",
    "        if best_feature is None:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Split the training set into subgroups, based on whether the selected feature is greater than\n",
    "        # the threshold or not\n",
    "        X_low, X_high, Y_low, Y_high = self.split_by_feature(X, Y, best_feature, best_threshold)\n",
    "\n",
    "        # Build the subtrees using a recursive call. Each subtree is associated\n",
    "        # with a value of the feature.\n",
    "        low_subtree = self.make_tree(X_low, Y_low, max_depth - 1)\n",
    "        high_subtree = self.make_tree(X_high, Y_high, max_depth - 1)\n",
    "\n",
    "        if low_subtree == high_subtree:\n",
    "            return low_subtree\n",
    "\n",
    "        # Return a decision tree branch containing the result.\n",
    "        return DecisionTreeBranch(best_feature, best_threshold, low_subtree, high_subtree)\n",
    "\n",
    "    # Utility method that splits the data into the \"upper\" and \"lower\" part, based on a feature\n",
    "    # and a threshold.\n",
    "    def split_by_feature(self, X, Y, feature, threshold):\n",
    "        low = X[:, feature] <= threshold\n",
    "        high = ~low\n",
    "        return X[low], X[high], Y[low], Y[high]\n",
    "\n",
    "    # The following three methods need to be implemented by the classification and regression subclasses.\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_default_value(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_homogeneous(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def best_split(self, X, Y, feature):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:48.790217Z",
     "start_time": "2024-01-18T09:32:48.785138Z"
    }
   },
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:48.797725Z",
     "start_time": "2024-01-18T09:32:48.791377Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class TreeClassifier(DecisionTree, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, max_depth=10, criterion='maj_sum'):\n",
    "        super().__init__(max_depth)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # For decision tree classifiers, there are some different ways to measure\n",
    "        # the homogeneity of subsets.\n",
    "        if self.criterion == 'maj_sum':\n",
    "            self.criterion_function = majority_sum_scorer\n",
    "        elif self.criterion == 'info_gain':\n",
    "            self.criterion_function = info_gain_scorer\n",
    "        elif self.criterion == 'gini':\n",
    "            self.criterion_function = gini_scorer\n",
    "        else:\n",
    "            raise Exception(f'Unknown criterion: {self.criterion}')\n",
    "        super().fit(X, Y)\n",
    "        self.classes_ = sorted(set(Y))\n",
    "\n",
    "    # Select a default value that is going to be used if we decide to make a leaf.\n",
    "    # We will select the most common value.\n",
    "    def get_default_value(self, Y):\n",
    "        self.class_distribution = Counter(Y)\n",
    "        return self.class_distribution.most_common(1)[0][0]\n",
    "\n",
    "    # Checks whether a set of output values is homogeneous. In the classification case, \n",
    "    # this means that all output values are identical.\n",
    "    # We assume that we called get_default_value just before, so that we can access\n",
    "    # the class_distribution attribute. If the class distribution contains just one item,\n",
    "    # this means that the set is homogeneous.\n",
    "    def is_homogeneous(self, Y):\n",
    "        return len(self.class_distribution) == 1\n",
    "\n",
    "    # Finds the best splitting point for a given feature. We'll keep frequency tables (Counters)\n",
    "    # for the upper and lower parts, and then compute the impurity criterion using these tables.\n",
    "    # In the end, we return a triple consisting of\n",
    "    # - the best score we found, according to the criterion we're using\n",
    "    # - the id of the feature\n",
    "    # - the threshold for the best split\n",
    "    def best_split(self, X, Y, feature):\n",
    "        # Create a list of input-output pairs, where we have sorted\n",
    "        # in ascending order by the input feature we're considering.\n",
    "        sorted_indices = np.argsort(X[:, feature])\n",
    "        X_sorted = list(X[sorted_indices, feature])\n",
    "        Y_sorted = list(Y[sorted_indices])\n",
    "\n",
    "        n = len(Y)\n",
    "\n",
    "        # The frequency tables corresponding to the parts *before and including*\n",
    "        # and *after* the current element.\n",
    "        low_distr = Counter()\n",
    "        high_distr = Counter(Y)\n",
    "\n",
    "        # Keep track of the best result we've seen so far.\n",
    "        max_score = -np.inf\n",
    "        max_i = None\n",
    "\n",
    "        # Go through all the positions (excluding the last position).\n",
    "        for i in range(0, n - 1):\n",
    "\n",
    "            # Input and output at the current position.\n",
    "            x_i = X_sorted[i]\n",
    "            y_i = Y_sorted[i]\n",
    "\n",
    "            # Update the frequency tables.\n",
    "            low_distr[y_i] += 1\n",
    "            high_distr[y_i] -= 1\n",
    "\n",
    "            # If the input is equal to the input at the next position, we will\n",
    "            # not consider a split here.\n",
    "            #x_next = XY[i+1][0]\n",
    "            x_next = X_sorted[i + 1]\n",
    "            if x_i == x_next:\n",
    "                continue\n",
    "\n",
    "            # Compute the homogeneity criterion for a split at this position.\n",
    "            score = self.criterion_function(i + 1, low_distr, n - i - 1, high_distr)\n",
    "\n",
    "            # If this is the best split, remember it.\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_i = i\n",
    "\n",
    "        # If we didn't find any split (meaning that all inputs are identical), return\n",
    "        # a dummy value.\n",
    "        if max_i is None:\n",
    "            return -np.inf, None, None\n",
    "\n",
    "        # Otherwise, return the best split we found and its score.\n",
    "        split_point = 0.5 * (X_sorted[max_i] + X_sorted[max_i + 1])\n",
    "        return max_score, feature, split_point\n",
    "\n",
    "\n",
    "def majority_sum_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    maj_sum_low = low_distr.most_common(1)[0][1]\n",
    "    maj_sum_high = high_distr.most_common(1)[0][1]\n",
    "    return maj_sum_low + maj_sum_high\n",
    "\n",
    "\n",
    "def entropy(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i / n for n_i in distr.values()]\n",
    "    return -sum(p * np.log2(p) if p > 0 else 0 for p in ps)\n",
    "\n",
    "\n",
    "def info_gain_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low * entropy(low_distr) + n_high * entropy(high_distr)) / (n_low + n_high)\n",
    "\n",
    "\n",
    "def gini_impurity(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i / n for n_i in distr.values()]\n",
    "    return 1 - sum(p ** 2 for p in ps)\n",
    "\n",
    "\n",
    "def gini_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low * gini_impurity(low_distr) + n_high * gini_impurity(high_distr)) / (n_low + n_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and tuning the decision tree\n",
    "\n",
    "With the copied and pasted code settled, we can use the `TreeClassifier` class and fine tune it to bead the gradient boost classifier."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best depth: 7, with accuracy: 0.9341176470588234\n"
     ]
    }
   ],
   "source": [
    "best = (0, 0)\n",
    "for depth in range(1, 10):\n",
    "    clf = TreeClassifier(max_depth=depth, criterion='gini')\n",
    "    aggr = aggregate(cross_val_score(clf, Xtrain, Ytrain))\n",
    "    if aggr > best[1]:\n",
    "        best = (depth, aggr)\n",
    "print(f'Best depth: {best[0]}, with accuracy: {best[1]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:51.288459Z",
     "start_time": "2024-01-18T09:32:48.795035Z"
    }
   },
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9178403755868545\n"
     ]
    }
   ],
   "source": [
    "depth = best[0]\n",
    "clf = TreeClassifier(max_depth=depth, criterion='gini')\n",
    "clf.fit(Xtrain, Ytrain)\n",
    "Yguess = clf.predict(Xtest)\n",
    "print(f'Accuracy: {accuracy_score(Ytest, Yguess)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:51.388956Z",
     "start_time": "2024-01-18T09:32:51.289186Z"
    }
   },
   "execution_count": 106
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drawing the tree with graphviz\n",
    "\n",
    "We use a small max depth to visualize the decision tree."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n -->\n<!-- Pages: 1 -->\n<svg width=\"598pt\" height=\"310pt\"\n viewBox=\"0.00 0.00 598.07 309.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 305.5)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-305.5 594.07,-305.5 594.07,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"37.53\" cy=\"-18\" rx=\"37.53\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"37.53\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">normal</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"131.53\" cy=\"-18\" rx=\"38.04\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"131.53\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">suspect</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"yellow\" stroke=\"black\" d=\"M164.28,-124.5C164.28,-124.5 98.78,-124.5 98.78,-124.5 92.78,-124.5 86.78,-118.5 86.78,-112.5 86.78,-112.5 86.78,-100.5 86.78,-100.5 86.78,-94.5 92.78,-88.5 98.78,-88.5 98.78,-88.5 164.28,-88.5 164.28,-88.5 170.28,-88.5 176.28,-94.5 176.28,-100.5 176.28,-100.5 176.28,-112.5 176.28,-112.5 176.28,-118.5 170.28,-124.5 164.28,-124.5\"/>\n<text text-anchor=\"middle\" x=\"131.53\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">F7 &gt; 0.7249?</text>\n</g>\n<!-- 2&#45;&gt;0 -->\n<g id=\"edge1\" class=\"edge\">\n<title>2&#45;&gt;0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M112.96,-88.41C98.51,-75.12 78.33,-56.54 62.47,-41.95\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"65.02,-39.54 55.29,-35.34 60.28,-44.69 65.02,-39.54\"/>\n<text text-anchor=\"middle\" x=\"107.78\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 2&#45;&gt;1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>2&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M131.53,-88.41C131.53,-76.76 131.53,-61.05 131.53,-47.52\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"135.03,-47.86 131.53,-37.86 128.03,-47.86 135.03,-47.86\"/>\n<text text-anchor=\"middle\" x=\"144.28\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"237.53\" cy=\"-18\" rx=\"49.82\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"237.53\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">pathologic</text>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"342.53\" cy=\"-18\" rx=\"37.53\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"342.53\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">normal</text>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"yellow\" stroke=\"black\" d=\"M327.91,-124.5C327.91,-124.5 251.16,-124.5 251.16,-124.5 245.16,-124.5 239.16,-118.5 239.16,-112.5 239.16,-112.5 239.16,-100.5 239.16,-100.5 239.16,-94.5 245.16,-88.5 251.16,-88.5 251.16,-88.5 327.91,-88.5 327.91,-88.5 333.91,-88.5 339.91,-94.5 339.91,-100.5 339.91,-100.5 339.91,-112.5 339.91,-112.5 339.91,-118.5 333.91,-124.5 327.91,-124.5\"/>\n<text text-anchor=\"middle\" x=\"289.53\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">F3 &gt; &#45;0.05405?</text>\n</g>\n<!-- 5&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>5&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M279.26,-88.41C271.92,-76.21 261.91,-59.55 253.53,-45.62\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"256.72,-44.12 248.56,-37.35 250.72,-47.73 256.72,-44.12\"/>\n<text text-anchor=\"middle\" x=\"282.78\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 5&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>5&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M300,-88.41C307.53,-76.13 317.82,-59.34 326.39,-45.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"329.23,-47.41 331.47,-37.05 323.26,-43.75 329.23,-47.41\"/>\n<text text-anchor=\"middle\" x=\"334.28\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"yellow\" stroke=\"black\" d=\"M318.91,-213C318.91,-213 260.16,-213 260.16,-213 254.16,-213 248.16,-207 248.16,-201 248.16,-201 248.16,-189 248.16,-189 248.16,-183 254.16,-177 260.16,-177 260.16,-177 318.91,-177 318.91,-177 324.91,-177 330.91,-183 330.91,-189 330.91,-189 330.91,-201 330.91,-201 330.91,-207 324.91,-213 318.91,-213\"/>\n<text text-anchor=\"middle\" x=\"289.53\" y=\"-189.95\" font-family=\"Times,serif\" font-size=\"14.00\">F9 &gt; 3.173?</text>\n</g>\n<!-- 6&#45;&gt;2 -->\n<g id=\"edge5\" class=\"edge\">\n<title>6&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M257.94,-176.7C233.58,-163.37 199.69,-144.81 173.13,-130.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"175.12,-127.37 164.67,-125.64 171.76,-133.51 175.12,-127.37\"/>\n<text text-anchor=\"middle\" x=\"239.78\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 6&#45;&gt;5 -->\n<g id=\"edge6\" class=\"edge\">\n<title>6&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M289.53,-176.91C289.53,-165.26 289.53,-149.55 289.53,-136.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"293.03,-136.36 289.53,-126.36 286.03,-136.36 293.03,-136.36\"/>\n<text text-anchor=\"middle\" x=\"302.28\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"447.53\" cy=\"-18\" rx=\"49.82\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"447.53\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">pathologic</text>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"552.53\" cy=\"-18\" rx=\"37.53\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"552.53\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">normal</text>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"yellow\" stroke=\"black\" d=\"M480.28,-124.5C480.28,-124.5 414.78,-124.5 414.78,-124.5 408.78,-124.5 402.78,-118.5 402.78,-112.5 402.78,-112.5 402.78,-100.5 402.78,-100.5 402.78,-94.5 408.78,-88.5 414.78,-88.5 414.78,-88.5 480.28,-88.5 480.28,-88.5 486.28,-88.5 492.28,-94.5 492.28,-100.5 492.28,-100.5 492.28,-112.5 492.28,-112.5 492.28,-118.5 486.28,-124.5 480.28,-124.5\"/>\n<text text-anchor=\"middle\" x=\"447.53\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">F13 &gt; 3.146?</text>\n</g>\n<!-- 9&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>9&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M447.53,-88.41C447.53,-76.76 447.53,-61.05 447.53,-47.52\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"451.03,-47.86 447.53,-37.86 444.03,-47.86 451.03,-47.86\"/>\n<text text-anchor=\"middle\" x=\"461.78\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 9&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>9&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M468.53,-88.2C484.94,-74.68 507.87,-55.8 525.62,-41.17\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"527.71,-43.98 533.21,-34.92 523.26,-38.58 527.71,-43.98\"/>\n<text text-anchor=\"middle\" x=\"522.28\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"547.53\" cy=\"-106.5\" rx=\"37.53\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"547.53\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">normal</text>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"yellow\" stroke=\"black\" d=\"M482.53,-213C482.53,-213 412.53,-213 412.53,-213 406.53,-213 400.53,-207 400.53,-201 400.53,-201 400.53,-189 400.53,-189 400.53,-183 406.53,-177 412.53,-177 412.53,-177 482.53,-177 482.53,-177 488.53,-177 494.53,-183 494.53,-189 494.53,-189 494.53,-201 494.53,-201 494.53,-207 488.53,-213 482.53,-213\"/>\n<text text-anchor=\"middle\" x=\"447.53\" y=\"-189.95\" font-family=\"Times,serif\" font-size=\"14.00\">F17 &gt; &#45;1.722?</text>\n</g>\n<!-- 11&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>11&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M447.53,-176.91C447.53,-165.26 447.53,-149.55 447.53,-136.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"451.03,-136.36 447.53,-126.36 444.03,-136.36 451.03,-136.36\"/>\n<text text-anchor=\"middle\" x=\"461.78\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 11&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>11&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M467.29,-176.91C482.8,-163.49 504.53,-144.7 521.47,-130.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"523.41,-132.99 528.68,-123.81 518.83,-127.7 523.41,-132.99\"/>\n<text text-anchor=\"middle\" x=\"519.28\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"yellow\" stroke=\"black\" d=\"M377.16,-301.5C377.16,-301.5 313.91,-301.5 313.91,-301.5 307.91,-301.5 301.91,-295.5 301.91,-289.5 301.91,-289.5 301.91,-277.5 301.91,-277.5 301.91,-271.5 307.91,-265.5 313.91,-265.5 313.91,-265.5 377.16,-265.5 377.16,-265.5 383.16,-265.5 389.16,-271.5 389.16,-277.5 389.16,-277.5 389.16,-289.5 389.16,-289.5 389.16,-295.5 383.16,-301.5 377.16,-301.5\"/>\n<text text-anchor=\"middle\" x=\"345.53\" y=\"-278.45\" font-family=\"Times,serif\" font-size=\"14.00\">F8 &gt; &#45;0.886?</text>\n</g>\n<!-- 12&#45;&gt;6 -->\n<g id=\"edge11\" class=\"edge\">\n<title>12&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M334.47,-265.41C326.62,-253.29 315.93,-236.77 306.94,-222.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"309.96,-221.11 301.59,-214.62 304.08,-224.92 309.96,-221.11\"/>\n<text text-anchor=\"middle\" x=\"337.78\" y=\"-234.2\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 12&#45;&gt;11 -->\n<g id=\"edge12\" class=\"edge\">\n<title>12&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M365.68,-265.41C380.81,-252.58 401.74,-234.83 418.65,-220.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"420.56,-223.47 425.92,-214.33 416.03,-218.13 420.56,-223.47\"/>\n<text text-anchor=\"middle\" x=\"419.28\" y=\"-234.2\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.graphs.Digraph at 0x149ae4b50>"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = TreeClassifier(max_depth=3, criterion='gini')\n",
    "clf.fit(Xtrain, Ytrain)\n",
    "Yguess = clf.predict(Xtest)\n",
    "clf.draw_tree()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:32:51.635331Z",
     "start_time": "2024-01-18T09:32:51.389730Z"
    }
   },
   "execution_count": 107
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p align='center'>\n",
    "<b>Fig:</b> Decision tree produced by Graphviz with <code>max_depth=3</code>.\n",
    "</p>\n",
    "\n",
    "After trying out some different max-depths, the best one we got was a max-depth of 7, with an accuracy of `0.9178403755868545`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3. Predicting apartment prices\n",
    "\n",
    "We will now predict apartment prices using regression."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-18 10:32:51--  https://www.cse.chalmers.se/~richajo/dit866/data/sberbank.csv\r\n",
      "Resolving www.cse.chalmers.se (www.cse.chalmers.se)... 129.16.221.33\r\n",
      "Connecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.221.33|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 46690044 (45M) [text/plain]\r\n",
      "Saving to: ‘sberbank.csv.1’\r\n",
      "\r\n",
      "sberbank.csv.1      100%[===================>]  44.53M  4.08MB/s    in 11s     \r\n",
      "\r\n",
      "2024-01-18 10:33:02 (4.22 MB/s) - ‘sberbank.csv.1’ saved [46690044/46690044]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Fetch the sberbank dataset.\n",
    "!wget https://www.cse.chalmers.se/~richajo/dit866/data/sberbank.csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:33:02.767003Z",
     "start_time": "2024-01-18T09:32:51.634771Z"
    }
   },
   "execution_count": 108
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading and preprocessing the data\n",
    "\n",
    "With the above `wget` to the dataset we download the data. Then we use the provided snippet to read it preprocess it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read the CSV file using Pandas.\n",
    "alldata = pd.read_csv(\"sberbank.csv\")\n",
    "\n",
    "\n",
    "# Convert the timestamp string to an integer representing the year.\n",
    "def get_year(timestamp):\n",
    "    return int(timestamp[:4])\n",
    "\n",
    "\n",
    "alldata['year'] = alldata.timestamp.apply(get_year)\n",
    "\n",
    "# Select the 9 input columns and the output column.\n",
    "selected_columns = ['price_doc', 'year', 'full_sq', 'life_sq', 'floor', 'num_room', 'kitch_sq', 'full_all']\n",
    "alldata = alldata[selected_columns]\n",
    "alldata = alldata.dropna()\n",
    "\n",
    "# Shuffle.\n",
    "alldata_shuffled = alldata.sample(frac=1.0, random_state=0)\n",
    "\n",
    "# Separate the input and output columns.\n",
    "X = alldata_shuffled.drop('price_doc', axis=1)\n",
    "# For the output, we'll use the log of the sales price.\n",
    "Y = alldata_shuffled['price_doc'].apply(np.log)\n",
    "\n",
    "# Split into training and test sets.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:33:03.171217Z",
     "start_time": "2024-01-18T09:33:02.766107Z"
    }
   },
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       year  full_sq  life_sq  floor  num_room  kitch_sq  full_all\n25252  2014       61     32.0    8.0       2.0      13.0    247469\n9943   2013       43     20.0   10.0       1.0       8.0     68630\n18040  2014       56     30.0   11.0       2.0       8.0     78507\n8625   2013       54     32.0   10.0       2.0       9.0     26943\n13495  2013       38     20.0    2.0       1.0       8.0    132349",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>full_sq</th>\n      <th>life_sq</th>\n      <th>floor</th>\n      <th>num_room</th>\n      <th>kitch_sq</th>\n      <th>full_all</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25252</th>\n      <td>2014</td>\n      <td>61</td>\n      <td>32.0</td>\n      <td>8.0</td>\n      <td>2.0</td>\n      <td>13.0</td>\n      <td>247469</td>\n    </tr>\n    <tr>\n      <th>9943</th>\n      <td>2013</td>\n      <td>43</td>\n      <td>20.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>68630</td>\n    </tr>\n    <tr>\n      <th>18040</th>\n      <td>2014</td>\n      <td>56</td>\n      <td>30.0</td>\n      <td>11.0</td>\n      <td>2.0</td>\n      <td>8.0</td>\n      <td>78507</td>\n    </tr>\n    <tr>\n      <th>8625</th>\n      <td>2013</td>\n      <td>54</td>\n      <td>32.0</td>\n      <td>10.0</td>\n      <td>2.0</td>\n      <td>9.0</td>\n      <td>26943</td>\n    </tr>\n    <tr>\n      <th>13495</th>\n      <td>2013</td>\n      <td>38</td>\n      <td>20.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>132349</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check.\n",
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:33:03.176967Z",
     "start_time": "2024-01-18T09:33:03.173481Z"
    }
   },
   "execution_count": 110
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obtaining a baseline regressor\n",
    "\n",
    "We use a dummy regressor as a baseline to evaluate upcoming regressors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "-0.38925247260237567"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "m1 = DummyRegressor()\n",
    "aggregate(cross_val_score(m1, Xtrain, Ytrain, scoring='neg_mean_squared_error'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:33:03.187367Z",
     "start_time": "2024-01-18T09:33:03.176220Z"
    }
   },
   "execution_count": 111
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the provided classifiers, we find the best regression model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Ridge score: -0.30139784232179734\n",
      "name: Lasso score: -0.3010470671748873\n",
      "name: Linear Regression score: -0.3013986588767263\n",
      "name: Decision Tree score: -0.5271702424150605\n",
      "name: Random Forest score: -0.26899811081917646\n",
      "name: Gradient Boosting score: -0.26152817204690715\n",
      "name: Neural Network score: -7.6538789750490945\n",
      "Best Model: `Gradient Boosting` with score: -0.26152817204690715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create some regressors.\n",
    "regressors = {\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(max_depth=7, criterion=\"friedman_mse\"),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(criterion=\"friedman_mse\", max_depth=7, learning_rate=0.1),\n",
    "    \"Neural Network\": MLPRegressor(hidden_layer_sizes=(200,), activation='relu', solver='adam',\n",
    "                                   learning_rate_init=0.003, max_iter=20000, early_stopping=True)\n",
    "}\n",
    "\n",
    "# Find the best model.\n",
    "best_score = float('-inf')\n",
    "best_name = None\n",
    "for name, reg in regressors.items():\n",
    "    score = aggregate(cross_val_score(reg, Xtrain, Ytrain, scoring='neg_mean_squared_error'))\n",
    "    print(f\"name: {name} score: {score}\")\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_name = name\n",
    "\n",
    "print(f\"Best Model: `{best_name}` with score: {best_score}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:33:20.948111Z",
     "start_time": "2024-01-18T09:33:03.183660Z"
    }
   },
   "execution_count": 112
  },
  {
   "cell_type": "markdown",
   "source": [
    "The regression model we got the best reproducible score with was the `GradientBoostingRegressor`, which got a negative mse of `-0.2645212922213328`, without any additional hyperparameter fine-tuning.\n",
    "The regressor works in similar fashion to its corresponding classifier. It works additively by first making a baseline, initial model, and fits the data on it. Then it calculates residuals from the current model and the actual data. Once done calculating, a new model is formed to improve the previous one, the steps are repeated until a stopping criterion is met.\n",
    "It should be noted that the gradient boosting is based on minimizing the loss function which measures how well the model fits the data, which is important in evaluating how well the model performs.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4. Decision trees for regression\n",
    "\n",
    "In this task we implement our own regressor, heavily based on the tree classifier. The main difference is the criterion function, which we base on variance reduction.\n",
    "\n",
    "### Step 1. Implementing the regression model\n",
    "\n",
    "Below is the implementation of the regression model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin\n",
    "\n",
    "\n",
    "class TreeRegressor(DecisionTree, RegressorMixin):\n",
    "    def __init__(self, max_depth: int = 10, var_thresh: float = 0.1):\n",
    "        super().__init__(max_depth)\n",
    "        self.var_thresh = var_thresh\n",
    "\n",
    "    def get_default_value(self, Y):\n",
    "        return np.mean(Y)\n",
    "\n",
    "    def is_homogeneous(self, Y):\n",
    "        return np.var(Y) < self.var_thresh\n",
    "\n",
    "    def variance(self, Y):\n",
    "        n = len(Y)\n",
    "        sum_x = np.sum(Y)\n",
    "        sum_x2 = np.sum([x ** 2 for x in Y])\n",
    "        return (1 / n) * sum_x2 - (1 / (n ** 2)) * (sum_x ** 2)\n",
    "\n",
    "    def best_split(self, X, Y, feature):\n",
    "        # Create a list of input-output pairs, where we have sorted\n",
    "        # in ascending order by the input feature we're considering.\n",
    "        sorted_indices = np.argsort(X[:, feature])\n",
    "        X_sorted = list(X[sorted_indices, feature])\n",
    "        Y_sorted = list(Y[sorted_indices])\n",
    "\n",
    "        n = len(Y)\n",
    "\n",
    "        max_score = -np.inf\n",
    "        max_i = None\n",
    "\n",
    "        # Precompute total sums and variance.\n",
    "        total_y = np.sum(Y)\n",
    "        total_y2 = np.sum([x ** 2 for x in Y])\n",
    "        total_variance = (total_y2 - (total_y ** 2) / n) / n\n",
    "        low_sum, low_sq_sum, high_sum, high_sq_sum = 0, 0, total_y, total_y2\n",
    "\n",
    "        for i in range(0, n - 1):\n",
    "            # Keep track of all sums.\n",
    "            y_i = Y_sorted[i]\n",
    "            low_sum += y_i\n",
    "            low_sq_sum += y_i ** 2\n",
    "            high_sum -= y_i\n",
    "            high_sq_sum -= y_i ** 2\n",
    "\n",
    "            # Continue until it is different.\n",
    "            x_i, x_next = X_sorted[i], X_sorted[i + 1]\n",
    "            if x_i == x_next:\n",
    "                continue\n",
    "\n",
    "            # Look at the variance difference.\n",
    "            low_n, high_n = i + 1, n - i - 1\n",
    "            low_variance = (low_sq_sum - (low_sum ** 2) / low_n) / low_n\n",
    "            high_variance = (high_sq_sum - (high_sum ** 2) / high_n) / high_n\n",
    "\n",
    "            variance_reduction = total_variance - (low_variance * low_n + high_variance * high_n) / n\n",
    "\n",
    "            if variance_reduction > max_score:\n",
    "                max_score = variance_reduction\n",
    "                max_i = i\n",
    "\n",
    "        # If no split was found, return a dummy value.\n",
    "        if max_i is None:\n",
    "            return -np.inf, None, None\n",
    "\n",
    "        split_point = 0.5 * (X_sorted[max_i] + X_sorted[max_i + 1])\n",
    "        return max_score, feature, split_point\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        super().fit(X, Y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:33:20.988560Z",
     "start_time": "2024-01-18T09:33:20.950709Z"
    }
   },
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2. Sanity check\n",
    "\n",
    "Generate and plot a dummy dataset with one variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_some_data(n):\n",
    "    x = np.random.uniform(-5, 5, size=n)\n",
    "    Y = (x > 1) + 0.1 * np.random.normal(size=n)\n",
    "    X = x.reshape(n, 1)  # X needs to be a 2-dimensional matrix\n",
    "    return X, Y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:33:20.990648Z",
     "start_time": "2024-01-18T09:33:20.964122Z"
    }
   },
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model on the dummy dataset and display it with `graphviz`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n -->\n<!-- Pages: 1 -->\n<svg width=\"188pt\" height=\"133pt\"\n viewBox=\"0.00 0.00 187.70 132.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 128.5)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-128.5 183.7,-128.5 183.7,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"44.19\" cy=\"-18\" rx=\"44.19\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"44.19\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;0.01284</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"143.19\" cy=\"-18\" rx=\"36.51\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"143.19\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">0.9911</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"yellow\" stroke=\"black\" d=\"M122.56,-124.5C122.56,-124.5 63.81,-124.5 63.81,-124.5 57.81,-124.5 51.81,-118.5 51.81,-112.5 51.81,-112.5 51.81,-100.5 51.81,-100.5 51.81,-94.5 57.81,-88.5 63.81,-88.5 63.81,-88.5 122.56,-88.5 122.56,-88.5 128.56,-88.5 134.56,-94.5 134.56,-100.5 134.56,-100.5 134.56,-112.5 134.56,-112.5 134.56,-118.5 128.56,-124.5 122.56,-124.5\"/>\n<text text-anchor=\"middle\" x=\"93.19\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">F0 &gt; 1.006?</text>\n</g>\n<!-- 2&#45;&gt;0 -->\n<g id=\"edge1\" class=\"edge\">\n<title>2&#45;&gt;0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M83.51,-88.41C76.66,-76.33 67.34,-59.88 59.5,-46.03\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"62.57,-44.35 54.59,-37.37 56.48,-47.8 62.57,-44.35\"/>\n<text text-anchor=\"middle\" x=\"87.44\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 2&#45;&gt;1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>2&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.06,-88.41C110.16,-76.13 119.87,-59.34 127.96,-45.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"130.77,-47.48 132.74,-37.07 124.71,-43.98 130.77,-47.48\"/>\n<text text-anchor=\"middle\" x=\"134.94\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.graphs.Digraph at 0x14f690a90>"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = TreeRegressor(max_depth=3)\n",
    "X, Y = make_some_data(n=200)\n",
    "reg.fit(X, Y)\n",
    "reg.draw_tree()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:33:21.151840Z",
     "start_time": "2024-01-18T09:33:20.976708Z"
    }
   },
   "execution_count": 115
  },
  {
   "cell_type": "markdown",
   "source": [
    "Considering the data generating function we want a regression tree to predict the output from the input. We don't want to simply classify\n",
    "the data into two classes, we want a value.\n",
    "\n",
    "Nothing happens if we give it a bigger depth, the tree is only 1 node deep, even if we change the max_depth to a large value, the resulting model will still only be 1 node deep, due to how the data is distributed.  \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3. Predicting apartment prices\n",
    "\n",
    "With the dummy dataset working, let's train on the sberbank dataset. We will train the regressor for different max depths, test them, and plot the results.\n",
    "\n",
    "We trained a tree regression model with `max_depth=6` and achieved a negative mse of `-0.2806332424585957`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for depth in range(0, 12):\n",
    "    reg = TreeRegressor(max_depth=depth)\n",
    "    # Train the regressor.\n",
    "    reg.fit(Xtrain, Ytrain)\n",
    "\n",
    "    # Predict and save the results from the training data.\n",
    "    Ypred_train = reg.predict(Xtrain)\n",
    "    score = mean_squared_error(Ytrain, Ypred_train)\n",
    "    train_scores.append(score)\n",
    "\n",
    "    # Predict and save the results from the test set.\n",
    "    Ypred_test = reg.predict(Xtest)\n",
    "    score = mean_squared_error(Ytest, Ypred_test)\n",
    "    test_scores.append(score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:33:23.721394Z",
     "start_time": "2024-01-18T09:33:21.150819Z"
    }
   },
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data.\n",
    "plt.scatter([i for i in range(12)], test_scores, color='blue', label='Test scores')\n",
    "plt.scatter([i for i in range(12)], train_scores, color='red', label='Training scores')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('mse')\n",
    "plt.title('Scatter Plot of Test and Training scores')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T09:33:23.871633Z",
     "start_time": "2024-01-18T09:33:23.737103Z"
    }
   },
   "execution_count": 117
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the plot, we can see that the model is overfitting on the training set. The evalutation on the test set keeps improving until\n",
    "around `max_depth=6`, then the evaluation score really starts to diverge and we can see that it overfits on the training data because it gets better at the training set, but worse on the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
